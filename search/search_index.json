{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS Controllers for Kubernetes (ACK) \u00b6 The AWS Controllers for Kubernetes (ACK) will allow containerized applications and Kubernetes users to create, update, delete and retrieve the status of resources in AWS services such as S3 buckets, DynamoDB, RDS databases, SNS, etc. using the Kubernetes API, for example using Kubernetes manifests or kubectl plugins. ACK comprises a set of Kubernetes custom controllers . Each controller manages custom resources representing API resources of a single AWS service API. For example, the service controller for AWS Simple Storage Service (S3) manages custom resources that represent AWS S3 buckets, keys, etc. Instead of logging into the AWS console or using the aws CLI tool to interact with the AWS service API, Kubernetes users can install a controller for an AWS service and then create, update, read and delete AWS resources using the Kubernetes API. This means they can use the Kubernetes API to fully describe both their containerized applications, using Kubernetes resources like Deployment and Service , as well as any AWS managed services upon which those applications depend. We are currently in the MVP phase, see also the issue 22 for details. If you have feedback, questions, or suggestions please don't hesitate to submit an issue , a pull request or comment on an existing issue.","title":"Home"},{"location":"#aws-controllers-for-kubernetes-ack","text":"The AWS Controllers for Kubernetes (ACK) will allow containerized applications and Kubernetes users to create, update, delete and retrieve the status of resources in AWS services such as S3 buckets, DynamoDB, RDS databases, SNS, etc. using the Kubernetes API, for example using Kubernetes manifests or kubectl plugins. ACK comprises a set of Kubernetes custom controllers . Each controller manages custom resources representing API resources of a single AWS service API. For example, the service controller for AWS Simple Storage Service (S3) manages custom resources that represent AWS S3 buckets, keys, etc. Instead of logging into the AWS console or using the aws CLI tool to interact with the AWS service API, Kubernetes users can install a controller for an AWS service and then create, update, read and delete AWS resources using the Kubernetes API. This means they can use the Kubernetes API to fully describe both their containerized applications, using Kubernetes resources like Deployment and Service , as well as any AWS managed services upon which those applications depend. We are currently in the MVP phase, see also the issue 22 for details. If you have feedback, questions, or suggestions please don't hesitate to submit an issue , a pull request or comment on an existing issue.","title":"AWS Controllers for Kubernetes (ACK)"},{"location":"community/background/","text":"Background \u00b6 In 10/2018 Chris Hein introduced the AWS Service Operator (ASO) project. We reviewed the feedback from the community and stakeholders and in 08/2019 decided to relaunch ASO as a first-tier open source project with concrete commitments from the container service team. In this process, we renamed the project to AWS Controllers for Kubernetes (ACK). The tenets for the relaunch were: ACK is a community-driven project, based on a governance model defining roles and responsibilities. ACK is optimized for production usage with full test coverage including performance and scalability test suites. ACK strives to be the only codebase exposing AWS services via a Kubernetes operator. Since then, we worked on design issues and gathering feedback around which services to prioritize. Existing custom controllers \u00b6 AWS service teams use custom controllers, webhooks, and operators for different use cases and based on different approaches. Examples include: Sagemaker operator , allowing to use Sagemaker from Kubernetes App Mesh controller , managing App Mesh resources from Kubernetes EKS Pod Identity Webhook , providing IAM roles for service accounts functionality While the autonomy in the different teams and project allows for rapid iterations and innovations, there are some drawbacks associated with it: The UX differs and that can lead to frustration when adopting an offering. A consistent quality bar across the different offerings is hard to establish and to verify. It's wasteful to re-invent the plumbing and necessary infrastructure (testing, etc.). Above is the motivation for our 3rd tenet: we want to make sure that there is a common framework, implementing good practices as put forward, for example, in the Operator Developer Guide or in the Programming Kubernetes book. Related projects \u00b6 Outside of AWS, there are projects that share similar goals we have with the ASO, for example: Crossplane aws-s3-provisioner","title":"Background"},{"location":"community/background/#background","text":"In 10/2018 Chris Hein introduced the AWS Service Operator (ASO) project. We reviewed the feedback from the community and stakeholders and in 08/2019 decided to relaunch ASO as a first-tier open source project with concrete commitments from the container service team. In this process, we renamed the project to AWS Controllers for Kubernetes (ACK). The tenets for the relaunch were: ACK is a community-driven project, based on a governance model defining roles and responsibilities. ACK is optimized for production usage with full test coverage including performance and scalability test suites. ACK strives to be the only codebase exposing AWS services via a Kubernetes operator. Since then, we worked on design issues and gathering feedback around which services to prioritize.","title":"Background"},{"location":"community/background/#existing-custom-controllers","text":"AWS service teams use custom controllers, webhooks, and operators for different use cases and based on different approaches. Examples include: Sagemaker operator , allowing to use Sagemaker from Kubernetes App Mesh controller , managing App Mesh resources from Kubernetes EKS Pod Identity Webhook , providing IAM roles for service accounts functionality While the autonomy in the different teams and project allows for rapid iterations and innovations, there are some drawbacks associated with it: The UX differs and that can lead to frustration when adopting an offering. A consistent quality bar across the different offerings is hard to establish and to verify. It's wasteful to re-invent the plumbing and necessary infrastructure (testing, etc.). Above is the motivation for our 3rd tenet: we want to make sure that there is a common framework, implementing good practices as put forward, for example, in the Operator Developer Guide or in the Programming Kubernetes book.","title":"Existing custom controllers"},{"location":"community/background/#related-projects","text":"Outside of AWS, there are projects that share similar goals we have with the ASO, for example: Crossplane aws-s3-provisioner","title":"Related projects"},{"location":"community/discussions/","text":"Discussions \u00b6 For discussions, please use the #provider-aws channel on the Kubernetes Slack community or the mailing list .","title":"Discussions"},{"location":"community/discussions/#discussions","text":"For discussions, please use the #provider-aws channel on the Kubernetes Slack community or the mailing list .","title":"Discussions"},{"location":"community/faq/","text":"Frequently Asked Questions (FAQ) \u00b6 Service Broker \u00b6 Question Does ACK replace the service broker ? Answer For the time being, people using the service broker should continue to use it and we're coordinating with the maintainers to provide a unified solution. The service broker project is also an AWS activity that, with the general shift of focus in the community from service broker to operators, can be considered less actively developed. There are a certain things around application lifecycle management that the service broker currently covers and which are at this juncture not yet covered by the scope of ACK, however we expect in the mid to long run that these two projects converge. We had AWS-internal discussions with the team that maintains the service broker and we're on the same page concerning a unified solution. We appreciate input and advice concerning features that are currently covered by the service broker only, for example bind/unbind or cataloging and looking forward to learn from the community how they are using service broker so that we can take this into account. Contributing \u00b6 Question Where and how can I help? Answer Excellent question and we're super excited that you're interested in ACK. For now, if you're a developer, you can check out the mvp branch and try out the code generation.","title":"FAQ"},{"location":"community/faq/#frequently-asked-questions-faq","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"community/faq/#service-broker","text":"Question Does ACK replace the service broker ? Answer For the time being, people using the service broker should continue to use it and we're coordinating with the maintainers to provide a unified solution. The service broker project is also an AWS activity that, with the general shift of focus in the community from service broker to operators, can be considered less actively developed. There are a certain things around application lifecycle management that the service broker currently covers and which are at this juncture not yet covered by the scope of ACK, however we expect in the mid to long run that these two projects converge. We had AWS-internal discussions with the team that maintains the service broker and we're on the same page concerning a unified solution. We appreciate input and advice concerning features that are currently covered by the service broker only, for example bind/unbind or cataloging and looking forward to learn from the community how they are using service broker so that we can take this into account.","title":"Service Broker"},{"location":"community/faq/#contributing","text":"Question Where and how can I help? Answer Excellent question and we're super excited that you're interested in ACK. For now, if you're a developer, you can check out the mvp branch and try out the code generation.","title":"Contributing"},{"location":"dev-docs/code-generation/","text":"Code generation \u00b6 In order to keep the code for all the service controllers consistent, we will use a strategy of generating the custom resource definitions and controller code stubs for new AWS services. Options considered \u00b6 To generate custom resource (definitions) and controller stub code, we investigated a number of options: home-grown custom code generator kudo kubebuilder a hybrid custom code generator + sigs.kubernetes.io/controller-tools (CR) The original AWS Service Operator used a custom-built generator that processed YAML manifests describing the AWS service and used templates to generate CRDs , the controller code itself and the Go types that represent the CRDs in memory. It's worth noting that the CRDs and the controller code that was generated by the original ASO was very tightly coupled to CloudFormation. In fact, the CRDs for individual AWS services like S3 or RDS were thin wrappers around CloudFormation stacks that described the object being operated upon. kudo is a platform for building Kubernetes Operators. It stores state in its own kudo.dev CRDs and allows users to define \"plans\" for a deployed application to deploy itself. We determined that kudo was not a particularly good fit for ASO for a couple reasons. First, we needed a way to generate CRDs in several API groups (s3.aws.com and iam.aws.com for example) and the ASO controller code isn't deploying an \"application\" that needs to have a controlled deployment plan. Instead, ASO is a controller that facilitates creation and management of various AWS service objects using Kubernetes CRD instances. kubebuilder is the recommended upstream tool for generating CRDs and controller stub code. It is a Go binary that creates the scaffolding for CRDs and controller Go code. It has support for multiple API groups (e.g. s3.amazonaws.com and dynamodb.amazonaws.com ) in a single code repository, so allows for sensible separation of code. Our final option was to build a hybrid custom code generator that used controller-runtime under the hood but allowed us to generate controller stub code for multiple API groups and place generated code in directories that represented Go best practices. This option gives us the flexibility to generate the files and content for multiple API groups but still stay within the recommended guardrails of the upstream Kubernetes community. Our approach \u00b6 We ended up with a hybrid custom+controller-runtime, using multiple phases of code generation: The first code generation phase consumes model information from a canonical source of truth about an AWS service and the objects and interfaces that service exposes and generates one or more files containing code that exposes Go types for those objects. These \"type files\" should be annotated with the marker and comments that will allow the core code generators and controller-gen to do its work. Once we have generated the type files, we need to generate basic scaffolding for consumers of those types. The upstream code-generator project contains generators for this scaffolding. We should be able to make a modified version of the upstream generate-groups.sh script to generate all the defaults, deepcopy, informers, listers and clientset code. Next, we will need to generate some skeleton code for the reconciling controller handling the new API along with the YAML files representing the CRDs that will get loaded into kube-apiserver and the YAML files for RBAC and OpenAPI v3 schemas for the CRDs. This is where the controller-gen tool from the sigs.kubernetes.io/controller-tools project will come in handy. We also want to generate some stub code for a new reconciling controller into the primary aws-service-operator binary. When we run make generate $SERVICE $VERSION , we should end up with a directory structure like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 /cmd /aws-service-operator main.go /crds /$SERVICE crds.yaml rbac.yaml /pkg /apis /$SERVICE /$VERSION doc.go register.go types.go deepcopy.go defaults.go /client /clientset /versioned /fake ... /scheme ... /typed /$SERVICE /$VERSION \u2026 /controllers /$SERVICE controller.go /informers /externalversions /$SERVICE /$VERSION ... /internalinterfaces /listers /$SERVICE /$VERSION ...","title":"Code generation"},{"location":"dev-docs/code-generation/#code-generation","text":"In order to keep the code for all the service controllers consistent, we will use a strategy of generating the custom resource definitions and controller code stubs for new AWS services.","title":"Code generation"},{"location":"dev-docs/code-generation/#options-considered","text":"To generate custom resource (definitions) and controller stub code, we investigated a number of options: home-grown custom code generator kudo kubebuilder a hybrid custom code generator + sigs.kubernetes.io/controller-tools (CR) The original AWS Service Operator used a custom-built generator that processed YAML manifests describing the AWS service and used templates to generate CRDs , the controller code itself and the Go types that represent the CRDs in memory. It's worth noting that the CRDs and the controller code that was generated by the original ASO was very tightly coupled to CloudFormation. In fact, the CRDs for individual AWS services like S3 or RDS were thin wrappers around CloudFormation stacks that described the object being operated upon. kudo is a platform for building Kubernetes Operators. It stores state in its own kudo.dev CRDs and allows users to define \"plans\" for a deployed application to deploy itself. We determined that kudo was not a particularly good fit for ASO for a couple reasons. First, we needed a way to generate CRDs in several API groups (s3.aws.com and iam.aws.com for example) and the ASO controller code isn't deploying an \"application\" that needs to have a controlled deployment plan. Instead, ASO is a controller that facilitates creation and management of various AWS service objects using Kubernetes CRD instances. kubebuilder is the recommended upstream tool for generating CRDs and controller stub code. It is a Go binary that creates the scaffolding for CRDs and controller Go code. It has support for multiple API groups (e.g. s3.amazonaws.com and dynamodb.amazonaws.com ) in a single code repository, so allows for sensible separation of code. Our final option was to build a hybrid custom code generator that used controller-runtime under the hood but allowed us to generate controller stub code for multiple API groups and place generated code in directories that represented Go best practices. This option gives us the flexibility to generate the files and content for multiple API groups but still stay within the recommended guardrails of the upstream Kubernetes community.","title":"Options considered"},{"location":"dev-docs/code-generation/#our-approach","text":"We ended up with a hybrid custom+controller-runtime, using multiple phases of code generation: The first code generation phase consumes model information from a canonical source of truth about an AWS service and the objects and interfaces that service exposes and generates one or more files containing code that exposes Go types for those objects. These \"type files\" should be annotated with the marker and comments that will allow the core code generators and controller-gen to do its work. Once we have generated the type files, we need to generate basic scaffolding for consumers of those types. The upstream code-generator project contains generators for this scaffolding. We should be able to make a modified version of the upstream generate-groups.sh script to generate all the defaults, deepcopy, informers, listers and clientset code. Next, we will need to generate some skeleton code for the reconciling controller handling the new API along with the YAML files representing the CRDs that will get loaded into kube-apiserver and the YAML files for RBAC and OpenAPI v3 schemas for the CRDs. This is where the controller-gen tool from the sigs.kubernetes.io/controller-tools project will come in handy. We also want to generate some stub code for a new reconciling controller into the primary aws-service-operator binary. When we run make generate $SERVICE $VERSION , we should end up with a directory structure like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 /cmd /aws-service-operator main.go /crds /$SERVICE crds.yaml rbac.yaml /pkg /apis /$SERVICE /$VERSION doc.go register.go types.go deepcopy.go defaults.go /client /clientset /versioned /fake ... /scheme ... /typed /$SERVICE /$VERSION \u2026 /controllers /$SERVICE controller.go /informers /externalversions /$SERVICE /$VERSION ... /internalinterfaces /listers /$SERVICE /$VERSION ...","title":"Our approach"},{"location":"dev-docs/overview/","text":"Overview \u00b6 This section of the docs is for ACK contributors. The code generation section gives you a bit of background on how we go about automating the code generation for controllers and supporting artifacts. In the setup section we walk you through setting up your local Git environment with the repo and how advise you on how we handle contributions. Last but not least, in the testing section we show you how to test ACK locally.","title":"Overview"},{"location":"dev-docs/overview/#overview","text":"This section of the docs is for ACK contributors. The code generation section gives you a bit of background on how we go about automating the code generation for controllers and supporting artifacts. In the setup section we walk you through setting up your local Git environment with the repo and how advise you on how we handle contributions. Last but not least, in the testing section we show you how to test ACK locally.","title":"Overview"},{"location":"dev-docs/setup/","text":"Setup \u00b6 We walk you now through the setup to start contributing to the AWS Controller for Kubernetes (ACK). No matter if you're contributing code or docs, follow the steps below. Issue before PR Of course we're happy about code drops via PRs, however, in order to give us time to plan ahead and also to avoid disappointment, consider creating an issue first and submit a PR later. This also helps us to coordinate between different contributors and should in general help keeping everyone happy. Fork the upstream repository \u00b6 First, fork the upstream source repository into your personal GitHub account. Then, in $GOPATH/src/github.com/aws/ , clone your repo and add the upstream like so: 1 2 3 git clone git@github.com:$GITHUB_ID/aws-controllers-k8s && \\ cd aws-controllers-k8s && \\ git remote add upstream git@github.com:aws/aws-controllers-k8s Go version We recommend to use a Go version of 1.14 or above for development. Create your local branch \u00b6 Next, you create a local branch where you work on your feature or bug fix. Let's say you want to enhance the docs, so set BRANCH_NAME=docs-improve and then: 1 git fetch --all && git checkout -b $BRANCH_NAME upstream/main Commit changes \u00b6 Make your changes locally, commit and push using: 1 2 3 git commit - a - m \"improves the docs a lot\" git push origin $ BRANCH_NAME With an example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 4 /4 ) , 710 bytes | 710 .00 KiB/s, done . Total 4 ( delta 2 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 2 /2 ) , completed with 2 local objects. remote: This repository moved. Please use the new location: remote: git@github.com: $GITHUB_ID /aws-controllers-k8s.git remote: remote: Create a pull request for 'docs' on GitHub by visiting: remote: https://github.com/ $GITHUB_ID /aws-controllers-k8s/pull/new/docs remote: To github.com:a-hilaly/aws-controllers-k8s * [ new branch ] docs -> docs Create a pull request \u00b6 Finally, submit a pull request against the upstream source repository. Use either the link that show up as in the example above or to the upstream source repository and there open the pull request as depicted below: We monitor the GitHub repo and try to follow up with comments within a working day.","title":"Setup"},{"location":"dev-docs/setup/#setup","text":"We walk you now through the setup to start contributing to the AWS Controller for Kubernetes (ACK). No matter if you're contributing code or docs, follow the steps below. Issue before PR Of course we're happy about code drops via PRs, however, in order to give us time to plan ahead and also to avoid disappointment, consider creating an issue first and submit a PR later. This also helps us to coordinate between different contributors and should in general help keeping everyone happy.","title":"Setup"},{"location":"dev-docs/setup/#fork-the-upstream-repository","text":"First, fork the upstream source repository into your personal GitHub account. Then, in $GOPATH/src/github.com/aws/ , clone your repo and add the upstream like so: 1 2 3 git clone git@github.com:$GITHUB_ID/aws-controllers-k8s && \\ cd aws-controllers-k8s && \\ git remote add upstream git@github.com:aws/aws-controllers-k8s Go version We recommend to use a Go version of 1.14 or above for development.","title":"Fork the upstream repository"},{"location":"dev-docs/setup/#create-your-local-branch","text":"Next, you create a local branch where you work on your feature or bug fix. Let's say you want to enhance the docs, so set BRANCH_NAME=docs-improve and then: 1 git fetch --all && git checkout -b $BRANCH_NAME upstream/main","title":"Create your local branch"},{"location":"dev-docs/setup/#commit-changes","text":"Make your changes locally, commit and push using: 1 2 3 git commit - a - m \"improves the docs a lot\" git push origin $ BRANCH_NAME With an example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 4 /4 ) , 710 bytes | 710 .00 KiB/s, done . Total 4 ( delta 2 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 2 /2 ) , completed with 2 local objects. remote: This repository moved. Please use the new location: remote: git@github.com: $GITHUB_ID /aws-controllers-k8s.git remote: remote: Create a pull request for 'docs' on GitHub by visiting: remote: https://github.com/ $GITHUB_ID /aws-controllers-k8s/pull/new/docs remote: To github.com:a-hilaly/aws-controllers-k8s * [ new branch ] docs -> docs","title":"Commit changes"},{"location":"dev-docs/setup/#create-a-pull-request","text":"Finally, submit a pull request against the upstream source repository. Use either the link that show up as in the example above or to the upstream source repository and there open the pull request as depicted below: We monitor the GitHub repo and try to follow up with comments within a working day.","title":"Create a pull request"},{"location":"dev-docs/testing/","text":"Testing \u00b6 For local development and testing we use kind , which in turn requires Docker. To build and test an ACK controller with a kind cluster, execute the commands as described in the following from the root directory of your checked-out source repository . Footprint When you run the scripts/kind-build-test.sh script the first time, the step that builds the container image for the target ACK service controller can 40 or more minutes. This is because the container image contains a lot of dependencies. Once you successfully build the target image this base image layer is cached locally and the build takes a much shorter amount of time. We are aware of this (and the storage footprint, ca. 3 GB) and aim to reduce both in the fullness of time. Preparation \u00b6 To build the latest ack-generate binary, execute the following command: 1 make build-ack-generate Don't worry if you forget this, the script in the next step will complain with an ERROR: Unable to find an ack-generate binary message and give you another opportunity to rectify the situation. Build an ACK controller \u00b6 Define the service you want to build and test an ACK controller for by setting the SERVICE_TO_BUILD environment variable, in our case for Amazon ECR: 1 SERVICE_TO_BUILD=\"ecr\" Now we are in a position to generate the ACK service controller for the AWS ECR API and output the generated code to the services/$SERVICE_TO_BUILD directory: 1 ./scripts/build-controller.sh $SERVICE_TO_BUILD Above generates the custom resource definition (CRD) manifests for resources managed by that ACK service controller. It further generates the Helm chart that can be used to install those CRD manifests and a deployment manifest that runs the ACK service controller in a pod on a Kubernetes cluster (still TODO). Run tests \u00b6 Time to run the tests, so execute: 1 ./scripts/kind-build-test.sh -s $SERVICE_TO_BUILD This provisions a Kubernetes cluster using kind , builds a container image with the ACK service controller, and loads the container image into the kind cluster. It then installs the ACK service controller and related Kubernetes manifests into the kind cluster using kustomize build | kubectl apply -f - . Then, the above script runs a series of bash test scripts that call kubectl and the aws CLI tools to verify that custom resources of the type managed by the respective ACK service controller are created, updated and deleted appropriately (still TODO). Fianlly, the script deletes the kind cluster. You can prevent this last step from happening by passing the -p (for \"preserve\") flag to the scripts/kind-build-test.sh script. Tracking testing We track testing in the umbrella issue 6 . on GitHub. Use this issue as a starting point and if you create a new testing-related issue, mention it from there. Clean up test runs \u00b6 To clean up a kind Kubernetes cluster, which includes all the configuration files created by the script specifically for your test cluster, execute: 1 kind delete cluster --name $CLUSTER_NAME","title":"Testing"},{"location":"dev-docs/testing/#testing","text":"For local development and testing we use kind , which in turn requires Docker. To build and test an ACK controller with a kind cluster, execute the commands as described in the following from the root directory of your checked-out source repository . Footprint When you run the scripts/kind-build-test.sh script the first time, the step that builds the container image for the target ACK service controller can 40 or more minutes. This is because the container image contains a lot of dependencies. Once you successfully build the target image this base image layer is cached locally and the build takes a much shorter amount of time. We are aware of this (and the storage footprint, ca. 3 GB) and aim to reduce both in the fullness of time.","title":"Testing"},{"location":"dev-docs/testing/#preparation","text":"To build the latest ack-generate binary, execute the following command: 1 make build-ack-generate Don't worry if you forget this, the script in the next step will complain with an ERROR: Unable to find an ack-generate binary message and give you another opportunity to rectify the situation.","title":"Preparation"},{"location":"dev-docs/testing/#build-an-ack-controller","text":"Define the service you want to build and test an ACK controller for by setting the SERVICE_TO_BUILD environment variable, in our case for Amazon ECR: 1 SERVICE_TO_BUILD=\"ecr\" Now we are in a position to generate the ACK service controller for the AWS ECR API and output the generated code to the services/$SERVICE_TO_BUILD directory: 1 ./scripts/build-controller.sh $SERVICE_TO_BUILD Above generates the custom resource definition (CRD) manifests for resources managed by that ACK service controller. It further generates the Helm chart that can be used to install those CRD manifests and a deployment manifest that runs the ACK service controller in a pod on a Kubernetes cluster (still TODO).","title":"Build an ACK controller"},{"location":"dev-docs/testing/#run-tests","text":"Time to run the tests, so execute: 1 ./scripts/kind-build-test.sh -s $SERVICE_TO_BUILD This provisions a Kubernetes cluster using kind , builds a container image with the ACK service controller, and loads the container image into the kind cluster. It then installs the ACK service controller and related Kubernetes manifests into the kind cluster using kustomize build | kubectl apply -f - . Then, the above script runs a series of bash test scripts that call kubectl and the aws CLI tools to verify that custom resources of the type managed by the respective ACK service controller are created, updated and deleted appropriately (still TODO). Fianlly, the script deletes the kind cluster. You can prevent this last step from happening by passing the -p (for \"preserve\") flag to the scripts/kind-build-test.sh script. Tracking testing We track testing in the umbrella issue 6 . on GitHub. Use this issue as a starting point and if you create a new testing-related issue, mention it from there.","title":"Run tests"},{"location":"dev-docs/testing/#clean-up-test-runs","text":"To clean up a kind Kubernetes cluster, which includes all the configuration files created by the script specifically for your test cluster, execute: 1 kind delete cluster --name $CLUSTER_NAME","title":"Clean up test runs"},{"location":"user-docs/install/","text":"Install \u00b6 In the following we walk you through installing an AWS service controller. Helm (recommended) \u00b6 The recommended way to install an AWS service controller for Kubernetes is to use Helm 3. Before installing an AWS service controller, ensure you have added the AWS Controllers for Kubernetes Helm repository: 1 helm repo add ack https://aws.github.io/aws-service-operator-k8s Each AWS service controller is packaged into a separate container image, published on a public AWS Elastic Container Registry repository. Likewise, each AWS service controller has a separate Helm chart that installs\u2014as a Kubernetes Deployment \u2014the AWS service controller, necessary custom resource definitions (CRDs), Kubernetes RBAC manifests, and other supporting artifacts. You may install a particular AWS service controller using the helm install CLI command: 1 helm install [--namespace $KUBERNETES_NAMESPACE] ack/$SERVICE_ALIAS for example, if you wanted to install the AWS S3 service controller into the \"ack-system\" Kubernetes namespace, you would execute: 1 helm install --namespace ack-system ack/s3 Static Kubernetes manifests \u00b6 If you prefer not to use Helm, you may install a service controller using static Kubernetes manifests. Static Kubernetes manifests that install individual service controllers are attached as artifacts to releases of AWS Controllers for Kubernetes. Select a release from the list of releases for AWS Controllers for Kubernetes. You will see a list of Assets for the release. One of those Assets will be named services/$SERVICE_ALIAS/all-resources.yaml . For example, for the AWS S3 service controller, there will be an Asset named services/s3/all-resources.yaml attached to the release. Click on the link to download the YAML file. This YAML file may be fed to kubectl apply -f directly to install the service controller, any CRDs that it manages, and all necessary Kubernetes RBAC manifests. For example: 1 kubectl apply -f https://github.com/aws/aws-controllers-k8s/releases/download/v0.0.1/services/s3/all-resources.yaml Once you've installed one or more ACKs, make sure to configure permissions , next.","title":"Install"},{"location":"user-docs/install/#install","text":"In the following we walk you through installing an AWS service controller.","title":"Install"},{"location":"user-docs/install/#helm-recommended","text":"The recommended way to install an AWS service controller for Kubernetes is to use Helm 3. Before installing an AWS service controller, ensure you have added the AWS Controllers for Kubernetes Helm repository: 1 helm repo add ack https://aws.github.io/aws-service-operator-k8s Each AWS service controller is packaged into a separate container image, published on a public AWS Elastic Container Registry repository. Likewise, each AWS service controller has a separate Helm chart that installs\u2014as a Kubernetes Deployment \u2014the AWS service controller, necessary custom resource definitions (CRDs), Kubernetes RBAC manifests, and other supporting artifacts. You may install a particular AWS service controller using the helm install CLI command: 1 helm install [--namespace $KUBERNETES_NAMESPACE] ack/$SERVICE_ALIAS for example, if you wanted to install the AWS S3 service controller into the \"ack-system\" Kubernetes namespace, you would execute: 1 helm install --namespace ack-system ack/s3","title":"Helm (recommended)"},{"location":"user-docs/install/#static-kubernetes-manifests","text":"If you prefer not to use Helm, you may install a service controller using static Kubernetes manifests. Static Kubernetes manifests that install individual service controllers are attached as artifacts to releases of AWS Controllers for Kubernetes. Select a release from the list of releases for AWS Controllers for Kubernetes. You will see a list of Assets for the release. One of those Assets will be named services/$SERVICE_ALIAS/all-resources.yaml . For example, for the AWS S3 service controller, there will be an Asset named services/s3/all-resources.yaml attached to the release. Click on the link to download the YAML file. This YAML file may be fed to kubectl apply -f directly to install the service controller, any CRDs that it manages, and all necessary Kubernetes RBAC manifests. For example: 1 kubectl apply -f https://github.com/aws/aws-controllers-k8s/releases/download/v0.0.1/services/s3/all-resources.yaml Once you've installed one or more ACKs, make sure to configure permissions , next.","title":"Static Kubernetes manifests"},{"location":"user-docs/permissions/","text":"Configure permissions \u00b6 Because ACK bridges the Kubernetes and AWS APIs, before using ACK service controllers, you will need to do some initial configuration around Kubernetes and AWS Identity and Access Management (IAM) permissions. Configuring Kubernetes RBAC \u00b6 As part of installation, certain Kubernetes Role objects will be created that contain permissions to modify the Kubernetes custom resource (CR) objects that the ACK service controller is responsible for. NOTE : All Kubernetes CR objects managed by an ACK service controller are Namespaced objects; that is, there are no cluster-scoped ACK-managed CRs. By default, the following Kubernetes Role objects are created when installing an ACK service controller: ack.user : a Role used for reading and mutating namespace-scoped custom resource (CR) objects that the service controller manages. ack.reader : a Role used for reading namespaced-scoped custom resource (CR) objects that the service controller manages. When installing a service controller, if the Role already exists (because an ACK controller for a different AWS service has previously been installed), permissions to manage CRD and CR objects associated with the installed controller's AWS service are added to the existing Role . For example, if you installed the ACK service controller for AWS S3, during that installation process, the ack.user Role would have been granted read/write permissions to create CRs with a GroupKind of s3.services.k8s.aws/Bucket within a specific Kubernetes Namespace . Likewise the ack.reader Role would be been granted read permissions to view CRs with a GroupKind of s3.services.k8s.aws . If you later installed the ACK service controller for AWS SNS, the installation process would have added permissions to the ack.user Role to read/write CR objects of GroupKind sns.services.k8s.aws/Topic and added permissions to the ack.user Role to read CR objects of GroupKind sns.services.k8s.aws/Topic . If you would like to use a differently-named Kubernetes Role than the defaults, you are welcome to do so by modifying the Kubernetes manifests that are used as part of the installation process. Once the Kubernetes Role objects have been created, you will want to assign specific a Kubernetes User to a particular Role . You do this using the typical Kubernetes RoleBinding object. For example, assume you want to have the Kubernetes User named \"Alice\" have the ability to create, read, delete and modify CRs that ACK service controllers manage in the Kubernetes \"default\" Namespace , you would create a RoleBinding that looked like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack.user namespace : default subjects : - kind : User name : Alice apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : ack.user apiGroup : rbac.authorization.k8s.io Configuring AWS IAM \u00b6 Since ACK service controllers bridge the Kubernetes and AWS API worlds, in addition to configuring Kubernetes RBAC permissions, you will need to ensure that all AWS Identity and Access Management (IAM) roles and permissions have been properly created. TODO Cross-account resource management \u00b6 TODO","title":"Permissions"},{"location":"user-docs/permissions/#configure-permissions","text":"Because ACK bridges the Kubernetes and AWS APIs, before using ACK service controllers, you will need to do some initial configuration around Kubernetes and AWS Identity and Access Management (IAM) permissions.","title":"Configure permissions"},{"location":"user-docs/permissions/#configuring-kubernetes-rbac","text":"As part of installation, certain Kubernetes Role objects will be created that contain permissions to modify the Kubernetes custom resource (CR) objects that the ACK service controller is responsible for. NOTE : All Kubernetes CR objects managed by an ACK service controller are Namespaced objects; that is, there are no cluster-scoped ACK-managed CRs. By default, the following Kubernetes Role objects are created when installing an ACK service controller: ack.user : a Role used for reading and mutating namespace-scoped custom resource (CR) objects that the service controller manages. ack.reader : a Role used for reading namespaced-scoped custom resource (CR) objects that the service controller manages. When installing a service controller, if the Role already exists (because an ACK controller for a different AWS service has previously been installed), permissions to manage CRD and CR objects associated with the installed controller's AWS service are added to the existing Role . For example, if you installed the ACK service controller for AWS S3, during that installation process, the ack.user Role would have been granted read/write permissions to create CRs with a GroupKind of s3.services.k8s.aws/Bucket within a specific Kubernetes Namespace . Likewise the ack.reader Role would be been granted read permissions to view CRs with a GroupKind of s3.services.k8s.aws . If you later installed the ACK service controller for AWS SNS, the installation process would have added permissions to the ack.user Role to read/write CR objects of GroupKind sns.services.k8s.aws/Topic and added permissions to the ack.user Role to read CR objects of GroupKind sns.services.k8s.aws/Topic . If you would like to use a differently-named Kubernetes Role than the defaults, you are welcome to do so by modifying the Kubernetes manifests that are used as part of the installation process. Once the Kubernetes Role objects have been created, you will want to assign specific a Kubernetes User to a particular Role . You do this using the typical Kubernetes RoleBinding object. For example, assume you want to have the Kubernetes User named \"Alice\" have the ability to create, read, delete and modify CRs that ACK service controllers manage in the Kubernetes \"default\" Namespace , you would create a RoleBinding that looked like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack.user namespace : default subjects : - kind : User name : Alice apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : ack.user apiGroup : rbac.authorization.k8s.io","title":"Configuring Kubernetes RBAC"},{"location":"user-docs/permissions/#configuring-aws-iam","text":"Since ACK service controllers bridge the Kubernetes and AWS API worlds, in addition to configuring Kubernetes RBAC permissions, you will need to ensure that all AWS Identity and Access Management (IAM) roles and permissions have been properly created. TODO","title":"Configuring AWS IAM"},{"location":"user-docs/permissions/#cross-account-resource-management","text":"TODO","title":"Cross-account resource management"},{"location":"user-docs/usage/","text":"Usage \u00b6 In this section we discuss how to use AWS Controllers for Kubernetes. Prerequisites \u00b6 Before using AWS Controllers for Kubernetes, make sure to: install one or more ACK service controllers. configure permissions of Kubernetes and AWS Identity and Access Management Roles. Creating an AWS resource via the Kubernetes API \u00b6 TODO Viewing AWS resource information via the Kubernetes API \u00b6 TODO Deleting an AWS resource via the Kubernetes API \u00b6 TODO Modifying an AWS resource via the Kubernetes API \u00b6 TODO","title":"Usage"},{"location":"user-docs/usage/#usage","text":"In this section we discuss how to use AWS Controllers for Kubernetes.","title":"Usage"},{"location":"user-docs/usage/#prerequisites","text":"Before using AWS Controllers for Kubernetes, make sure to: install one or more ACK service controllers. configure permissions of Kubernetes and AWS Identity and Access Management Roles.","title":"Prerequisites"},{"location":"user-docs/usage/#creating-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Creating an AWS resource via the Kubernetes API"},{"location":"user-docs/usage/#viewing-aws-resource-information-via-the-kubernetes-api","text":"TODO","title":"Viewing AWS resource information via the Kubernetes API"},{"location":"user-docs/usage/#deleting-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Deleting an AWS resource via the Kubernetes API"},{"location":"user-docs/usage/#modifying-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Modifying an AWS resource via the Kubernetes API"}]}